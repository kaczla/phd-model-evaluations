model_name	depth	method	top_k	PerplexityHashed
ALBERT-base	1	simple	15	614059.204648
ALBERT-large	1	simple	15	581610.442096
ALBERT-xlarge	1	simple	15	613916.347819
ALBERT-xxlarge	1	simple	15	554443.961328
BERT-base-cased	1	simple	15	648649.056905
BERT-base-multilingual-cased	1	simple	15	660782.043255
BERT-base-multilingual-uncased	1	simple	15	617002.994761
BERT-base-uncased	1	simple	15	628889.202772
BERT-large-cased	1	simple	15	671036.949159
BERT-large-uncased	1	simple	15	627363.327835
BERT-medium-uncased	1	simple	15	741863.914526
BERT-mini-uncased	1	simple	15	719229.936894
BERT-small-uncased	1	simple	15	731435.070894
BERT-tiny-uncased	1	simple	15	790783.205389
BioGPT	1		15	535426.297827
BioMed-RoBERTa-base	1	simple	15	573382.436717
ByT5-base	1		15	866141.698694
ByT5-small	1		15	724136.469382
CamemBERT-base	1	simple	15	876848.870517
Cerebras-GPT-111M	1		15	650841.371093
Cerebras-GPT-256M	1		15	667994.532652
ClinicalBERT	1	simple	15	711138.755240
CodeBERT-base	1	simple	15	851860.732369
DistilBERT-base-cased	1	simple	15	616948.066589
DistilBERT-base-uncased	1	simple	15	600238.895928
DistilGPT-2	1		15	627795.725203
DistilRoBERTa-base	1	simple	15	483072.284047
FLAN-T5-base	1		15	871948.085526
FLAN-T5-large	1		15	886211.460601
FLAN-T5-small	1		15	927646.585326
FinBERT	1	simple	15	615715.972389
GPT-2-base	1		15	601879.149936
GPT-2-large	1		15	629274.272272
GPT-2-medium	1		15	647987.910467
GPT-Neo-125M	1		15	672276.313913
GPT-fr-base	1		15	831228.647031
GPT-fr-small	1		15	813796.815060
German-BERT-base-cased	1	simple	15	877233.717835
LongT5-Local-base	1		15	1023999.997628
LongT5-TGlobal-base	1		15	925297.699403
MiniLM-L12-H384-RoBERTa-large	1	simple	15	833469.587792
MiniLM-L12-H384-XLMR-Large	1	simple	15	955396.547824
MiniLM-L6-H384-BERT-base-uncased	1	simple	15	833055.323432
MiniLM-L6-H384-BERT-large-uncased	1	simple	15	955159.437222
MiniLM-L6-H384-RoBERTa-large	1	simple	15	923647.060664
MiniLM-L6-H384-XLMR-Large	1	simple	15	891513.143197
MiniLM-L6-H768-BERT-base-uncased	1	simple	15	989916.259213
MiniLM-L6-H768-BERT-large-uncased	1	simple	15	894077.955018
MiniLM-L6-H768-RoBERTa-large	1	simple	15	832537.928977
MobileBERT-uncased	1	simple	15	692725.915064
OPT-125M	1		15	704510.302739
OPT-350M	1		15	693103.877356
PolishGPT-2-large	1		15	835476.856578
PolishGPT-2-medium	1		15	836861.567483
PolishGPT-2-small	1		15	889976.526720
PolishRoBERT-base	1	simple	15	805776.952214
Pythia-160M	1		15	716023.419961
Pythia-160M-deduped	1		15	667899.600585
Pythia-410M	1		15	711271.056858
Pythia-410M-deduped	1		15	733570.420257
Pythia-70M	1		15	678984.098545
Pythia-70M-deduped	1		15	725395.902985
RoBERTa-base	1	simple	15	479300.877953
RoBERTa-large	1	simple	15	412110.227778
SciBERT-cased	1	simple	15	640146.995539
SciBERT-uncased	1	simple	15	667459.102739
SportsBERT	1	simple	15	889696.335657
Switch-base-8	1		15	702790.895322
T5-base	1		15	652216.477174
T5-base-v1.1	1		15	759094.825267
T5-base-v1.1-lm-adapt	1		15	875880.234607
T5-efficient-base	1		15	807858.010747
T5-efficient-large	1		15	742391.169757
T5-efficient-mini	1		15	772807.309414
T5-efficient-small	1		15	773965.298823
T5-efficient-tiny	1		15	773129.825407
T5-large	1		15	617531.342864
T5-large-v1.1	1		15	714707.720199
T5-large-v1.1-lm-adapt	1		15	770887.711635
T5-small	1		15	599997.250618
T5-small-v1.1	1		15	771407.358377
T5-small-v1.1-lm-adapt	1		15	892135.172720
XLM-100-lang	1	simple	15	820202.252782
XLM-17-lang	1	simple	15	738530.401196
XLM-RoBERTa-base	1	simple	15	871677.992311
XLM-RoBERTa-large	1	simple	15	850639.882073
XLM-en	1	simple	15	928153.440400
mT5-base	1		15	651157.591085
mT5-large	1		15	845762.461451
mT5-small	1		15	776925.182407
