{
    "model_list": [
        "RoBERTa-base",
        "RoBERTa-large",
        "BERT-base-cased",
        "BERT-base-uncased",
        "BERT-large-cased",
        "BERT-large-uncased",
        "BERT-tiny-uncased",
        "BERT-mini-uncased",
        "BERT-small-uncased",
        "BERT-medium-uncased",
        "DistilRoBERTa-base",
        "DistilBERT-base-cased",
        "DistilBERT-base-uncased",
        "ALBERT-base",
        "ALBERT-large",
        "MobileBERT-uncased",
        "FinBERT",
        "SciBERT-uncased",
        "SciBERT-cased",
        "BioMed-RoBERTa-base",
        "ClinicalBERT",
        "CodeBERT-base",
        "XLM-RoBERTa-base",
        "XLM-RoBERTa-large",
        "BERT-base-multilingual-uncased",
        "BERT-base-multilingual-cased",
        "Longformer-base",
        "Longformer-large",
        "CamemBERT-base",
        "PolishRoBERT-base",
        "German-BERT-base-cased",
        "GPT-2-base",
        "GPT-2-medium",
        "GPT-Neo-125M",
        "Pythia-70M",
        "Pythia-70M-deduped",
        "Pythia-160M",
        "Pythia-160M-deduped",
        "Pythia-410M",
        "Pythia-410M-deduped",
        "OPT-125M",
        "OPT-350M",
        "Cerebras-GPT-111M",
        "Cerebras-GPT-256M",
        "DistilGPT-2",
        "BioGPT",
        "GPT-fr-small",
        "PolishGPT-2-small",
        "T5-small",
        "T5-base",
        "T5-small-v1.1",
        "T5-base-v1.1",
        "T5-small-v1.1-lm-adapt",
        "T5-base-v1.1-lm-adapt",
        "T5-efficient-tiny",
        "T5-efficient-mini",
        "T5-efficient-small",
        "T5-efficient-base",
        "Switch-base-8",
        "FLAN-T5-small",
        "FLAN-T5-base",
        "mT5-small",
        "mT5-base",
        "ByT5-small",
        "ByT5-base",
        "LongT5-TGlobal-base",
        "LongT5-Local-base"
    ],
    "metrics": {
        "CoLA": "Matthews correlation",
        "MNLI-m": "Accuracy",
        "MNLI-mm": "Accuracy",
        "MRPC": "F1 score",
        "QNLI": "Accuracy",
        "QQP": "F1 score",
        "RTE": "Accuracy",
        "SST-2": "Accuracy",
        "STS-B": "Spearman correlation",
        "AVG": "Average score",
        "LM-GAP": "PerplexityHashed",
        "loss": "CrossEntropy Loss"
    },
    "results": {
        "CoLA": {
            "RoBERTa-base": 59.83,
            "RoBERTa-large": 66.36,
            "BERT-base-cased": 59.4,
            "BERT-base-uncased": 58.88,
            "BERT-large-cased": 64.46,
            "BERT-large-uncased": 62.26,
            "BERT-tiny-uncased": null,
            "BERT-mini-uncased": 24.08,
            "BERT-small-uncased": 38.71,
            "BERT-medium-uncased": 45.1,
            "DistilRoBERTa-base": 55.97,
            "DistilBERT-base-cased": 46.71,
            "DistilBERT-base-uncased": 54.15,
            "ALBERT-base": 54.3,
            "ALBERT-large": 56.03,
            "MobileBERT-uncased": 50.65,
            "FinBERT": 33.12,
            "SciBERT-uncased": 41.22,
            "SciBERT-cased": 37.13,
            "BioMed-RoBERTa-base": 54.22,
            "ClinicalBERT": 41.4,
            "CodeBERT-base": 40.12,
            "XLM-RoBERTa-base": 52.73,
            "XLM-RoBERTa-large": 59.2,
            "BERT-base-multilingual-uncased": 44.39,
            "BERT-base-multilingual-cased": 38.71,
            "Longformer-base": 60.5,
            "Longformer-large": 62.44,
            "CamemBERT-base": null,
            "PolishRoBERT-base": null,
            "German-BERT-base-cased": null,
            "GPT-2-base": 44.97,
            "GPT-2-medium": 54.51,
            "GPT-Neo-125M": 41.85,
            "Pythia-70M": null,
            "Pythia-70M-deduped": 21.64,
            "Pythia-160M": 31.73,
            "Pythia-160M-deduped": 35.84,
            "Pythia-410M": 42.73,
            "Pythia-410M-deduped": 48.74,
            "OPT-125M": 52.17,
            "OPT-350M": 53.08,
            "Cerebras-GPT-111M": 31.96,
            "Cerebras-GPT-256M": 39.68,
            "DistilGPT-2": 33.24,
            "BioGPT": 30.67,
            "GPT-fr-small": 20.34,
            "PolishGPT-2-small": null,
            "T5-small": 25.67,
            "T5-base": 57.78,
            "T5-small-v1.1": 1.2,
            "T5-base-v1.1": 2.6,
            "T5-small-v1.1-lm-adapt": 12.43,
            "T5-base-v1.1-lm-adapt": 41.41,
            "T5-efficient-tiny": 5.11,
            "T5-efficient-mini": null,
            "T5-efficient-small": 0.43,
            "T5-efficient-base": 2.01,
            "Switch-base-8": 47.46,
            "FLAN-T5-small": 4.94,
            "FLAN-T5-base": 43.36,
            "mT5-small": 1.54,
            "mT5-base": 4.5,
            "ByT5-small": 4.94,
            "ByT5-base": null,
            "LongT5-TGlobal-base": null,
            "LongT5-Local-base": null
        },
        "MNLI-m": {
            "RoBERTa-base": 83.43,
            "RoBERTa-large": 86.63,
            "BERT-base-cased": 81.13,
            "BERT-base-uncased": 81.68,
            "BERT-large-cased": 82.07,
            "BERT-large-uncased": 84.35,
            "BERT-tiny-uncased": 67.5,
            "BERT-mini-uncased": 74.1,
            "BERT-small-uncased": 76.08,
            "BERT-medium-uncased": 79.98,
            "DistilRoBERTa-base": 80.45,
            "DistilBERT-base-cased": 78.79,
            "DistilBERT-base-uncased": 80.49,
            "ALBERT-base": 79.16,
            "ALBERT-large": 80.07,
            "MobileBERT-uncased": 82.19,
            "FinBERT": 77.41,
            "SciBERT-uncased": 80.95,
            "SciBERT-cased": 79.71,
            "BioMed-RoBERTa-base": 84.06,
            "ClinicalBERT": 79.02,
            "CodeBERT-base": 80.45,
            "XLM-RoBERTa-base": 79.21,
            "XLM-RoBERTa-large": 83.74,
            "BERT-base-multilingual-uncased": 79.4,
            "BERT-base-multilingual-cased": 79.25,
            "Longformer-base": 84.5,
            "Longformer-large": 86.64,
            "CamemBERT-base": 78.93,
            "PolishRoBERT-base": 72.44,
            "German-BERT-base-cased": 73.26,
            "GPT-2-base": 81.01,
            "GPT-2-medium": 84.71,
            "GPT-Neo-125M": 78.83,
            "Pythia-70M": 69.19,
            "Pythia-70M-deduped": 72.54,
            "Pythia-160M": 74.12,
            "Pythia-160M-deduped": 74.61,
            "Pythia-410M": 77.61,
            "Pythia-410M-deduped": 77.22,
            "OPT-125M": 72.66,
            "OPT-350M": 74.75,
            "Cerebras-GPT-111M": 74.33,
            "Cerebras-GPT-256M": 74.18,
            "DistilGPT-2": 78.16,
            "BioGPT": 76.87,
            "GPT-fr-small": 72.28,
            "PolishGPT-2-small": 68.54,
            "T5-small": 80.7,
            "T5-base": 86.71,
            "T5-small-v1.1": 77.97,
            "T5-base-v1.1": 87.3,
            "T5-small-v1.1-lm-adapt": 79.51,
            "T5-base-v1.1-lm-adapt": 87.68,
            "T5-efficient-tiny": 70.03,
            "T5-efficient-mini": 75.21,
            "T5-efficient-small": 78.77,
            "T5-efficient-base": 84.2,
            "Switch-base-8": 86.44,
            "FLAN-T5-small": 80.85,
            "FLAN-T5-base": 87.74,
            "mT5-small": 75.06,
            "mT5-base": 82.85,
            "ByT5-small": 79.55,
            "ByT5-base": 83.29,
            "LongT5-TGlobal-base": 85.26,
            "LongT5-Local-base": 84.19
        },
        "MNLI-mm": {
            "RoBERTa-base": 84.23,
            "RoBERTa-large": 86.47,
            "BERT-base-cased": 81.28,
            "BERT-base-uncased": 82.35,
            "BERT-large-cased": 83.38,
            "BERT-large-uncased": 84.43,
            "BERT-tiny-uncased": 68.57,
            "BERT-mini-uncased": 75.5,
            "BERT-small-uncased": 76.93,
            "BERT-medium-uncased": 80.62,
            "DistilRoBERTa-base": 80.98,
            "DistilBERT-base-cased": 80.3,
            "DistilBERT-base-uncased": 81.31,
            "ALBERT-base": 79.51,
            "ALBERT-large": 80.65,
            "MobileBERT-uncased": 82.49,
            "FinBERT": 78.35,
            "SciBERT-uncased": 81.16,
            "SciBERT-cased": 80.11,
            "BioMed-RoBERTa-base": 84.45,
            "ClinicalBERT": 80.76,
            "CodeBERT-base": 81.55,
            "XLM-RoBERTa-base": 80.54,
            "XLM-RoBERTa-large": 83.86,
            "BERT-base-multilingual-uncased": 80.44,
            "BERT-base-multilingual-cased": 80.34,
            "Longformer-base": 84.7,
            "Longformer-large": 86.59,
            "CamemBERT-base": 79.14,
            "PolishRoBERT-base": 73.32,
            "German-BERT-base-cased": 74.12,
            "GPT-2-base": 81.92,
            "GPT-2-medium": 84.98,
            "GPT-Neo-125M": 80.07,
            "Pythia-70M": 70.47,
            "Pythia-70M-deduped": 73.91,
            "Pythia-160M": 75.39,
            "Pythia-160M-deduped": 75.46,
            "Pythia-410M": 78.58,
            "Pythia-410M-deduped": 78.91,
            "OPT-125M": 73.48,
            "OPT-350M": 76.76,
            "Cerebras-GPT-111M": 75.46,
            "Cerebras-GPT-256M": 75.73,
            "DistilGPT-2": 79.47,
            "BioGPT": 78.64,
            "GPT-fr-small": 72.88,
            "PolishGPT-2-small": 70.03,
            "T5-small": 81.81,
            "T5-base": 86.78,
            "T5-small-v1.1": 78.68,
            "T5-base-v1.1": 87.13,
            "T5-small-v1.1-lm-adapt": 80.47,
            "T5-base-v1.1-lm-adapt": 87.44,
            "T5-efficient-tiny": 71.43,
            "T5-efficient-mini": 75.84,
            "T5-efficient-small": 79.09,
            "T5-efficient-base": 84.33,
            "Switch-base-8": 86.44,
            "FLAN-T5-small": 81.18,
            "FLAN-T5-base": 87.44,
            "mT5-small": 76.22,
            "mT5-base": 83.76,
            "ByT5-small": 80.68,
            "ByT5-base": 83.91,
            "LongT5-TGlobal-base": 85.8,
            "LongT5-Local-base": 84.51
        },
        "MRPC": {
            "RoBERTa-base": 91.13,
            "RoBERTa-large": 93.66,
            "BERT-base-cased": 89.98,
            "BERT-base-uncased": 89.29,
            "BERT-large-cased": 91.26,
            "BERT-large-uncased": 89.19,
            "BERT-tiny-uncased": 81.78,
            "BERT-mini-uncased": 84.05,
            "BERT-small-uncased": 85.03,
            "BERT-medium-uncased": 86.71,
            "DistilRoBERTa-base": 89.9,
            "DistilBERT-base-cased": 88.32,
            "DistilBERT-base-uncased": 89.07,
            "ALBERT-base": 91.17,
            "ALBERT-large": 92.82,
            "MobileBERT-uncased": 89.75,
            "FinBERT": 86.73,
            "SciBERT-uncased": 90.78,
            "SciBERT-cased": 89.48,
            "BioMed-RoBERTa-base": 90.75,
            "ClinicalBERT": 88.52,
            "CodeBERT-base": 87.39,
            "XLM-RoBERTa-base": 91.26,
            "XLM-RoBERTa-large": 81.22,
            "BERT-base-multilingual-uncased": 90.05,
            "BERT-base-multilingual-cased": 90.53,
            "Longformer-base": 92.74,
            "Longformer-large": 81.22,
            "CamemBERT-base": 88.42,
            "PolishRoBERT-base": 89.19,
            "German-BERT-base-cased": 86.69,
            "GPT-2-base": 86.61,
            "GPT-2-medium": 87.89,
            "GPT-Neo-125M": 83.54,
            "Pythia-70M": 82.89,
            "Pythia-70M-deduped": 82.7,
            "Pythia-160M": 84.05,
            "Pythia-160M-deduped": 84.47,
            "Pythia-410M": 87.73,
            "Pythia-410M-deduped": 86.87,
            "OPT-125M": 68.28,
            "OPT-350M": 85.67,
            "Cerebras-GPT-111M": 84.72,
            "Cerebras-GPT-256M": 85.37,
            "DistilGPT-2": 86.84,
            "BioGPT": 85.99,
            "GPT-fr-small": 85.76,
            "PolishGPT-2-small": 84.77,
            "T5-small": 87.71,
            "T5-base": 91.67,
            "T5-small-v1.1": null,
            "T5-base-v1.1": 29.78,
            "T5-small-v1.1-lm-adapt": 81.22,
            "T5-base-v1.1-lm-adapt": 84.34,
            "T5-efficient-tiny": null,
            "T5-efficient-mini": null,
            "T5-efficient-small": 81.22,
            "T5-efficient-base": 22.42,
            "Switch-base-8": 81.29,
            "FLAN-T5-small": 82.52,
            "FLAN-T5-base": 86.68,
            "mT5-small": null,
            "mT5-base": 47.03,
            "ByT5-small": 81.22,
            "ByT5-base": 81.22,
            "LongT5-TGlobal-base": 82.02,
            "LongT5-Local-base": 82.11
        },
        "QNLI": {
            "RoBERTa-base": 91.4,
            "RoBERTa-large": 89.96,
            "BERT-base-cased": 89.64,
            "BERT-base-uncased": 91.08,
            "BERT-large-cased": 90.17,
            "BERT-large-uncased": 91.08,
            "BERT-tiny-uncased": 78.43,
            "BERT-mini-uncased": 84.54,
            "BERT-small-uncased": 86.53,
            "BERT-medium-uncased": 88.83,
            "DistilRoBERTa-base": 90.51,
            "DistilBERT-base-cased": 87.41,
            "DistilBERT-base-uncased": 88.41,
            "ALBERT-base": 90.73,
            "ALBERT-large": 88.77,
            "MobileBERT-uncased": 91.02,
            "FinBERT": 85.87,
            "SciBERT-uncased": 88.69,
            "SciBERT-cased": 89.05,
            "BioMed-RoBERTa-base": 91.43,
            "ClinicalBERT": 87.42,
            "CodeBERT-base": 88.64,
            "XLM-RoBERTa-base": 89.9,
            "XLM-RoBERTa-large": 88.61,
            "BERT-base-multilingual-uncased": 88.82,
            "BERT-base-multilingual-cased": 90.31,
            "Longformer-base": 91.76,
            "Longformer-large": 90.48,
            "CamemBERT-base": 87.92,
            "PolishRoBERT-base": 83.16,
            "German-BERT-base-cased": 82.63,
            "GPT-2-base": 87.79,
            "GPT-2-medium": 90.43,
            "GPT-Neo-125M": 86.09,
            "Pythia-70M": 79.83,
            "Pythia-70M-deduped": 79.79,
            "Pythia-160M": 83.42,
            "Pythia-160M-deduped": 84.1,
            "Pythia-410M": 87.1,
            "Pythia-410M-deduped": 85.67,
            "OPT-125M": 87.12,
            "OPT-350M": 85.96,
            "Cerebras-GPT-111M": 83.43,
            "Cerebras-GPT-256M": 84.48,
            "DistilGPT-2": 86.25,
            "BioGPT": 87.76,
            "GPT-fr-small": 83.01,
            "PolishGPT-2-small": 80.9,
            "T5-small": 90.43,
            "T5-base": 93.15,
            "T5-small-v1.1": 83.46,
            "T5-base-v1.1": 91.07,
            "T5-small-v1.1-lm-adapt": 85.02,
            "T5-base-v1.1-lm-adapt": 91.41,
            "T5-efficient-tiny": 80.44,
            "T5-efficient-mini": 84.07,
            "T5-efficient-small": 86.63,
            "T5-efficient-base": 90.28,
            "Switch-base-8": 91.14,
            "FLAN-T5-small": 90.62,
            "FLAN-T5-base": 93.02,
            "mT5-small": 84.77,
            "mT5-base": 90.02,
            "ByT5-small": 87.83,
            "ByT5-base": 91.52,
            "LongT5-TGlobal-base": 89.95,
            "LongT5-Local-base": 85.77
        },
        "QQP": {
            "RoBERTa-base": 85.25,
            "RoBERTa-large": 82.05,
            "BERT-base-cased": 84.31,
            "BERT-base-uncased": 85.8,
            "BERT-large-cased": 81.12,
            "BERT-large-uncased": 81.69,
            "BERT-tiny-uncased": 78.86,
            "BERT-mini-uncased": 81.5,
            "BERT-small-uncased": 83.11,
            "BERT-medium-uncased": 83.58,
            "DistilRoBERTa-base": 84.63,
            "DistilBERT-base-cased": 84.61,
            "DistilBERT-base-uncased": 84.17,
            "ALBERT-base": 83.67,
            "ALBERT-large": 80.41,
            "MobileBERT-uncased": 86.5,
            "FinBERT": 80.74,
            "SciBERT-uncased": 81.98,
            "SciBERT-cased": 82.03,
            "BioMed-RoBERTa-base": 84.75,
            "ClinicalBERT": 85.02,
            "CodeBERT-base": 84.53,
            "XLM-RoBERTa-base": 83.75,
            "XLM-RoBERTa-large": 81.75,
            "BERT-base-multilingual-uncased": 84.1,
            "BERT-base-multilingual-cased": 78.39,
            "Longformer-base": 80.22,
            "Longformer-large": 82.4,
            "CamemBERT-base": 84.74,
            "PolishRoBERT-base": 82.54,
            "German-BERT-base-cased": 78.28,
            "GPT-2-base": 85.47,
            "GPT-2-medium": 87.17,
            "GPT-Neo-125M": 83.1,
            "Pythia-70M": 80.78,
            "Pythia-70M-deduped": 79.25,
            "Pythia-160M": 76.84,
            "Pythia-160M-deduped": 80.36,
            "Pythia-410M": 80.9,
            "Pythia-410M-deduped": 79.4,
            "OPT-125M": 81.36,
            "OPT-350M": 78.83,
            "Cerebras-GPT-111M": 78.77,
            "Cerebras-GPT-256M": 77.39,
            "DistilGPT-2": 82.24,
            "BioGPT": 85.41,
            "GPT-fr-small": 80.92,
            "PolishGPT-2-small": 72.29,
            "T5-small": 84.11,
            "T5-base": 88.14,
            "T5-small-v1.1": 72.46,
            "T5-base-v1.1": 85.89,
            "T5-small-v1.1-lm-adapt": 82.2,
            "T5-base-v1.1-lm-adapt": 86.85,
            "T5-efficient-tiny": 72.68,
            "T5-efficient-mini": 66.47,
            "T5-efficient-small": 70.01,
            "T5-efficient-base": 86.17,
            "Switch-base-8": 87.21,
            "FLAN-T5-small": 79.46,
            "FLAN-T5-base": 81.74,
            "mT5-small": 81.16,
            "mT5-base": 85.38,
            "ByT5-small": 84.5,
            "ByT5-base": 87.94,
            "LongT5-TGlobal-base": 86.82,
            "LongT5-Local-base": 82.28
        },
        "RTE": {
            "RoBERTa-base": 76.49,
            "RoBERTa-large": 83.73,
            "BERT-base-cased": 70.16,
            "BERT-base-uncased": 69.62,
            "BERT-large-cased": 73.78,
            "BERT-large-uncased": 76.49,
            "BERT-tiny-uncased": 63.47,
            "BERT-mini-uncased": 66.37,
            "BERT-small-uncased": 64.56,
            "BERT-medium-uncased": 63.83,
            "DistilRoBERTa-base": 68.17,
            "DistilBERT-base-cased": 58.05,
            "DistilBERT-base-uncased": 64.92,
            "ALBERT-base": 73.24,
            "ALBERT-large": 81.19,
            "MobileBERT-uncased": 64.92,
            "FinBERT": 63.11,
            "SciBERT-uncased": 67.45,
            "SciBERT-cased": 64.56,
            "BioMed-RoBERTa-base": 73.6,
            "ClinicalBERT": 63.83,
            "CodeBERT-base": 63.83,
            "XLM-RoBERTa-base": 70.71,
            "XLM-RoBERTa-large": 81.56,
            "BERT-base-multilingual-uncased": 70.16,
            "BERT-base-multilingual-cased": 72.88,
            "Longformer-base": 76.49,
            "Longformer-large": 69.98,
            "CamemBERT-base": 69.98,
            "PolishRoBERT-base": 66.37,
            "German-BERT-base-cased": 62.39,
            "GPT-2-base": 64.56,
            "GPT-2-medium": 71.07,
            "GPT-Neo-125M": 65.64,
            "Pythia-70M": 64.56,
            "Pythia-70M-deduped": 62.03,
            "Pythia-160M": 66.0,
            "Pythia-160M-deduped": 67.45,
            "Pythia-410M": 68.9,
            "Pythia-410M-deduped": 70.16,
            "OPT-125M": 58.59,
            "OPT-350M": 70.52,
            "Cerebras-GPT-111M": 65.64,
            "Cerebras-GPT-256M": 68.17,
            "DistilGPT-2": 66.37,
            "BioGPT": 62.03,
            "GPT-fr-small": 63.83,
            "PolishGPT-2-small": 62.21,
            "T5-small": 67.63,
            "T5-base": 81.19,
            "T5-small-v1.1": 52.62,
            "T5-base-v1.1": 52.62,
            "T5-small-v1.1-lm-adapt": 61.66,
            "T5-base-v1.1-lm-adapt": 67.81,
            "T5-efficient-tiny": 54.79,
            "T5-efficient-mini": 52.62,
            "T5-efficient-small": 58.77,
            "T5-efficient-base": 63.47,
            "Switch-base-8": 62.39,
            "FLAN-T5-small": 68.17,
            "FLAN-T5-base": 82.64,
            "mT5-small": 52.62,
            "mT5-base": 57.69,
            "ByT5-small": 57.69,
            "ByT5-base": 55.52,
            "LongT5-TGlobal-base": 60.22,
            "LongT5-Local-base": 59.86
        },
        "SST-2": {
            "RoBERTa-base": 95.4,
            "RoBERTa-large": 96.2,
            "BERT-base-cased": 92.17,
            "BERT-base-uncased": 92.41,
            "BERT-large-cased": 93.1,
            "BERT-large-uncased": 93.44,
            "BERT-tiny-uncased": 80.78,
            "BERT-mini-uncased": 86.31,
            "BERT-small-uncased": 88.15,
            "BERT-medium-uncased": 90.22,
            "DistilRoBERTa-base": 92.87,
            "DistilBERT-base-cased": 91.14,
            "DistilBERT-base-uncased": 90.45,
            "ALBERT-base": 92.17,
            "ALBERT-large": 94.71,
            "MobileBERT-uncased": 90.79,
            "FinBERT": 87.57,
            "SciBERT-uncased": 89.76,
            "SciBERT-cased": 89.76,
            "BioMed-RoBERTa-base": 92.52,
            "ClinicalBERT": 89.87,
            "CodeBERT-base": 91.14,
            "XLM-RoBERTa-base": 93.1,
            "XLM-RoBERTa-large": 95.28,
            "BERT-base-multilingual-uncased": 91.25,
            "BERT-base-multilingual-cased": 89.99,
            "Longformer-base": 94.48,
            "Longformer-large": 96.43,
            "CamemBERT-base": 88.15,
            "PolishRoBERT-base": 85.04,
            "German-BERT-base-cased": 86.31,
            "GPT-2-base": 92.17,
            "GPT-2-medium": 94.25,
            "GPT-Neo-125M": 89.41,
            "Pythia-70M": 87.11,
            "Pythia-70M-deduped": 87.23,
            "Pythia-160M": 91.02,
            "Pythia-160M-deduped": 89.64,
            "Pythia-410M": 92.52,
            "Pythia-410M-deduped": 92.64,
            "OPT-125M": 91.94,
            "OPT-350M": 93.67,
            "Cerebras-GPT-111M": 87.57,
            "Cerebras-GPT-256M": 90.56,
            "DistilGPT-2": 89.99,
            "BioGPT": 89.99,
            "GPT-fr-small": 83.43,
            "PolishGPT-2-small": 82.74,
            "T5-small": 90.68,
            "T5-base": 94.59,
            "T5-small-v1.1": 87.69,
            "T5-base-v1.1": 92.87,
            "T5-small-v1.1-lm-adapt": 90.45,
            "T5-base-v1.1-lm-adapt": 94.36,
            "T5-efficient-tiny": 82.05,
            "T5-efficient-mini": 84.58,
            "T5-efficient-small": 89.18,
            "T5-efficient-base": 92.06,
            "Switch-base-8": 94.25,
            "FLAN-T5-small": 91.25,
            "FLAN-T5-base": 94.36,
            "mT5-small": 59.84,
            "mT5-base": 89.3,
            "ByT5-small": 62.83,
            "ByT5-base": 90.45,
            "LongT5-TGlobal-base": 92.98,
            "LongT5-Local-base": 91.6
        },
        "STS-B": {
            "RoBERTa-base": 90.84,
            "RoBERTa-large": 92.14,
            "BERT-base-cased": 89.71,
            "BERT-base-uncased": 89.36,
            "BERT-large-cased": 89.81,
            "BERT-large-uncased": 90.11,
            "BERT-tiny-uncased": 81.84,
            "BERT-mini-uncased": 85.99,
            "BERT-small-uncased": 87.13,
            "BERT-medium-uncased": 88.37,
            "DistilRoBERTa-base": 88.44,
            "DistilBERT-base-cased": 84.75,
            "DistilBERT-base-uncased": 86.98,
            "ALBERT-base": 90.67,
            "ALBERT-large": 91.12,
            "MobileBERT-uncased": 87.46,
            "FinBERT": 85.79,
            "SciBERT-uncased": 87.95,
            "SciBERT-cased": 88.5,
            "BioMed-RoBERTa-base": 90.11,
            "ClinicalBERT": 87.98,
            "CodeBERT-base": 87.86,
            "XLM-RoBERTa-base": 88.8,
            "XLM-RoBERTa-large": 90.94,
            "BERT-base-multilingual-uncased": 88.98,
            "BERT-base-multilingual-cased": 88.96,
            "Longformer-base": 91.1,
            "Longformer-large": 91.89,
            "CamemBERT-base": 85.71,
            "PolishRoBERT-base": 84.79,
            "German-BERT-base-cased": 84.52,
            "GPT-2-base": 85.94,
            "GPT-2-medium": 89.0,
            "GPT-Neo-125M": 83.52,
            "Pythia-70M": 78.17,
            "Pythia-70M-deduped": 80.66,
            "Pythia-160M": 86.11,
            "Pythia-160M-deduped": 86.47,
            "Pythia-410M": 88.34,
            "Pythia-410M-deduped": 88.73,
            "OPT-125M": 87.77,
            "OPT-350M": 88.11,
            "Cerebras-GPT-111M": 82.63,
            "Cerebras-GPT-256M": 85.06,
            "DistilGPT-2": 81.49,
            "BioGPT": 87.19,
            "GPT-fr-small": 79.54,
            "PolishGPT-2-small": 81.66,
            "T5-small": 85.03,
            "T5-base": 89.51,
            "T5-small-v1.1": 4.55,
            "T5-base-v1.1": 2.0,
            "T5-small-v1.1-lm-adapt": 31.7,
            "T5-base-v1.1-lm-adapt": 87.34,
            "T5-efficient-tiny": 9.96,
            "T5-efficient-mini": 13.18,
            "T5-efficient-small": 55.16,
            "T5-efficient-base": 65.73,
            "Switch-base-8": 86.15,
            "FLAN-T5-small": 58.0,
            "FLAN-T5-base": 89.05,
            "mT5-small": 83.72,
            "mT5-base": 85.67,
            "ByT5-small": 3.99,
            "ByT5-base": 80.41,
            "LongT5-TGlobal-base": 83.96,
            "LongT5-Local-base": 79.53
        },
        "AVG": {
            "RoBERTa-base": 84.22,
            "RoBERTa-large": 86.36,
            "BERT-base-cased": 81.98,
            "BERT-base-uncased": 82.27,
            "BERT-large-cased": 83.24,
            "BERT-large-uncased": 83.67,
            "BERT-tiny-uncased": 75.15,
            "BERT-mini-uncased": 73.6,
            "BERT-small-uncased": 76.25,
            "BERT-medium-uncased": 78.58,
            "DistilRoBERTa-base": 81.32,
            "DistilBERT-base-cased": 77.79,
            "DistilBERT-base-uncased": 79.99,
            "ALBERT-base": 81.62,
            "ALBERT-large": 82.86,
            "MobileBERT-uncased": 80.64,
            "FinBERT": 75.41,
            "SciBERT-uncased": 78.88,
            "SciBERT-cased": 77.81,
            "BioMed-RoBERTa-base": 82.88,
            "ClinicalBERT": 78.2,
            "CodeBERT-base": 78.39,
            "XLM-RoBERTa-base": 81.11,
            "XLM-RoBERTa-large": 82.91,
            "BERT-base-multilingual-uncased": 79.73,
            "BERT-base-multilingual-cased": 78.82,
            "Longformer-base": 84.06,
            "Longformer-large": 83.12,
            "CamemBERT-base": 82.87,
            "PolishRoBERT-base": 79.61,
            "German-BERT-base-cased": 78.52,
            "GPT-2-base": 78.94,
            "GPT-2-medium": 82.67,
            "GPT-Neo-125M": 76.89,
            "Pythia-70M": 76.62,
            "Pythia-70M-deduped": 71.08,
            "Pythia-160M": 74.3,
            "Pythia-160M-deduped": 75.38,
            "Pythia-410M": 78.27,
            "Pythia-410M-deduped": 78.7,
            "OPT-125M": 74.82,
            "OPT-350M": 78.6,
            "Cerebras-GPT-111M": 73.83,
            "Cerebras-GPT-256M": 75.62,
            "DistilGPT-2": 76.01,
            "BioGPT": 76.06,
            "GPT-fr-small": 71.33,
            "PolishGPT-2-small": 75.39,
            "T5-small": 77.09,
            "T5-base": 85.5,
            "T5-small-v1.1": 57.33,
            "T5-base-v1.1": 59.03,
            "T5-small-v1.1-lm-adapt": 67.19,
            "T5-base-v1.1-lm-adapt": 80.96,
            "T5-efficient-tiny": 55.81,
            "T5-efficient-mini": 64.57,
            "T5-efficient-small": 66.58,
            "T5-efficient-base": 65.63,
            "Switch-base-8": 80.31,
            "FLAN-T5-small": 70.78,
            "FLAN-T5-base": 82.89,
            "mT5-small": 64.37,
            "mT5-base": 69.58,
            "ByT5-small": 60.36,
            "ByT5-base": 81.78,
            "LongT5-TGlobal-base": 83.38,
            "LongT5-Local-base": 81.23
        },
        "LM-GAP": {
            "ALBERT-base": 873.14,
            "ALBERT-large": 499.56,
            "ALBERT-xlarge": 376.8,
            "ALBERT-xxlarge": 380.98,
            "BERT-base-cased": 112.66,
            "BERT-base-multilingual-cased": 450.92,
            "BERT-base-multilingual-uncased": 822.97,
            "BERT-base-uncased": 409.78,
            "BERT-large-cased": 81.63,
            "BERT-large-uncased": 337.62,
            "BERT-medium-uncased": 664.25,
            "BERT-mini-uncased": 1590.24,
            "BERT-small-uncased": 866.43,
            "BERT-tiny-uncased": 4818.0,
            "BioGPT": 6510.86,
            "BioMed-RoBERTa-base": 214.82,
            "ByT5-base": 864938.78,
            "ByT5-small": 543397.55,
            "CamemBERT-base": 3059.58,
            "Cerebras-GPT-111M": 4721.07,
            "Cerebras-GPT-256M": 3374.71,
            "ClinicalBERT": 2557.58,
            "CodeBERT-base": 823.8,
            "DistilBERT-base-cased": 338.86,
            "DistilBERT-base-uncased": 727.45,
            "DistilGPT-2": 5085.33,
            "DistilRoBERTa-base": 123.07,
            "FLAN-T5-base": 107563.62,
            "FLAN-T5-large": 545044.04,
            "FLAN-T5-small": 299116.34,
            "FinBERT": 2015.12,
            "GPT-2-base": 2657.47,
            "GPT-2-large": 1344.74,
            "GPT-2-medium": 1461.79,
            "GPT-Neo-125M": 2697.76,
            "GPT-fr-base": 36089.45,
            "GPT-fr-small": 28510.27,
            "German-BERT-base-cased": 23101.76,
            "LongT5-Local-base": 1007622.29,
            "LongT5-TGlobal-base": 178123.75,
            "MiniLM-L12-H384-RoBERTa-large": 857838.53,
            "MiniLM-L12-H384-XLMR-Large": 866537.7,
            "MiniLM-L6-H384-BERT-base-uncased": 869806.44,
            "MiniLM-L6-H384-BERT-large-uncased": 870307.94,
            "MiniLM-L6-H384-RoBERTa-large": 868185.08,
            "MiniLM-L6-H384-XLMR-Large": 869608.95,
            "MiniLM-L6-H768-BERT-base-uncased": 881467.09,
            "MiniLM-L6-H768-BERT-large-uncased": 868204.49,
            "MiniLM-L6-H768-RoBERTa-large": 870961.96,
            "MobileBERT-uncased": 467.12,
            "OPT-125M": 1507.01,
            "OPT-350M": 1288.8,
            "PolishGPT-2-large": 38975.11,
            "PolishGPT-2-medium": 46822.73,
            "PolishGPT-2-small": 61419.11,
            "PolishRoBERT-base": 11157.49,
            "Pythia-160M": 3099.18,
            "Pythia-160M-deduped": 3221.15,
            "Pythia-410M": 2141.05,
            "Pythia-410M-deduped": 2332.1,
            "Pythia-70M": 4949.82,
            "Pythia-70M-deduped": 5418.18,
            "RoBERTa-base": 61.59,
            "RoBERTa-large": 45.42,
            "SciBERT-cased": 1089.73,
            "SciBERT-uncased": 892.1,
            "SportsBERT": 2830.94,
            "Switch-base-8": 220.17,
            "T5-base": 189.42,
            "T5-base-v1.1": 304544.66,
            "T5-base-v1.1-lm-adapt": 75130.61,
            "T5-efficient-base": 159789.57,
            "T5-efficient-large": 103135.63,
            "T5-efficient-mini": 472262.96,
            "T5-efficient-small": 286624.99,
            "T5-efficient-tiny": 378679.72,
            "T5-large": 126.57,
            "T5-large-v1.1": 451047.04,
            "T5-large-v1.1-lm-adapt": 369989.9,
            "T5-small": 518.19,
            "T5-small-v1.1": 298092.36,
            "T5-small-v1.1-lm-adapt": 257670.12,
            "XLM-100-lang": 6068.37,
            "XLM-17-lang": 25321.3,
            "XLM-RoBERTa-base": 135.35,
            "XLM-RoBERTa-large": 94.18,
            "XLM-en": 394432.04,
            "mT5-base": 1484.09,
            "mT5-large": 1064.04,
            "mT5-small": 2854.31
        },
        "loss": {
            "ALBERT-base": 2.74,
            "ALBERT-large": 1.3,
            "ALBERT-xlarge": 1.34,
            "ALBERT-xxlarge": 1.92,
            "BERT-base-cased": 2.36,
            "BERT-base-multilingual-cased": 2.08,
            "BERT-base-multilingual-uncased": 2.19,
            "BERT-base-uncased": 2.33,
            "BERT-large-cased": 2.89,
            "BERT-large-uncased": 2.82,
            "BERT-medium-uncased": 1.7,
            "BERT-mini-uncased": 1.7,
            "BERT-small-uncased": 1.7,
            "BERT-tiny-uncased": 1.06,
            "BioGPT": 3.92,
            "BioMed-RoBERTa-base": 1.16,
            "ByT5-base": 2.7,
            "ByT5-small": 11.64,
            "CamemBERT-base": 1.61,
            "Cerebras-GPT-111M": 4.11,
            "Cerebras-GPT-256M": 3.83,
            "ClinicalBERT": 2.21,
            "CodeBERT-base": 1.62,
            "DistilBERT-base-cased": 2.56,
            "DistilBERT-base-uncased": 2.12,
            "DistilGPT-2": 3.95,
            "DistilRoBERTa-base": 0.68,
            "FLAN-T5-base": 1.6,
            "FLAN-T5-large": 0.83,
            "FLAN-T5-small": 2.19,
            "FinBERT": 3.01,
            "GPT-2-base": 3.67,
            "GPT-2-large": 3.35,
            "GPT-2-medium": 3.46,
            "GPT-Neo-125M": 3.74,
            "GPT-fr-base": 4.52,
            "GPT-fr-small": 4.6,
            "German-BERT-base-cased": 2.41,
            "German-GPT-2": 3.59,
            "German-GPT-2-larger": 3.65,
            "LongT5-Local-base": 380.89,
            "LongT5-Local-large": 300.85,
            "LongT5-TGlobal-base": 0.53,
            "LongT5-TGlobal-large": 325.17,
            "Longformer-base": 1.06,
            "Longformer-large": 0.32,
            "MiniLM-L12-H384-RoBERTa-large": 11.18,
            "MiniLM-L12-H384-XLMR-Large": 12.43,
            "MiniLM-L6-H384-BERT-base-uncased": 11.36,
            "MiniLM-L6-H384-BERT-large-uncased": 11.0,
            "MiniLM-L6-H384-RoBERTa-large": 10.94,
            "MiniLM-L6-H384-XLMR-Large": 12.43,
            "MiniLM-L6-H768-BERT-base-uncased": 11.97,
            "MiniLM-L6-H768-BERT-large-uncased": 10.77,
            "MiniLM-L6-H768-RoBERTa-large": 11.56,
            "MobileBERT-uncased": 2.13,
            "OPT-125M": 4.21,
            "OPT-350M": 4.08,
            "PolishGPT-2-large": 3.17,
            "PolishGPT-2-medium": 3.36,
            "PolishGPT-2-small": 3.66,
            "PolishRoBERT-base": 0.59,
            "Pythia-160M": 3.71,
            "Pythia-160M-deduped": 3.66,
            "Pythia-410M": 3.37,
            "Pythia-410M-deduped": 3.36,
            "Pythia-70M": 4.07,
            "Pythia-70M-deduped": 4.04,
            "RoBERTa-base": 0.95,
            "RoBERTa-large": 0.27,
            "SciBERT-cased": 1.8,
            "SciBERT-uncased": 1.69,
            "Switch-base-8": 22.21,
            "T5-base": 14.95,
            "T5-base-v1.1": 49.56,
            "T5-base-v1.1-lm-adapt": 5.62,
            "T5-efficient-base": 11.84,
            "T5-efficient-mini": 15.98,
            "T5-efficient-small": 6.61,
            "T5-efficient-tiny": 18.0,
            "T5-large": 14.65,
            "T5-large-v1.1": 23.1,
            "T5-small": 9.02,
            "T5-small-v1.1": 11.32,
            "T5-small-v1.1-lm-adapt": 3.13,
            "XLM-RoBERTa-base": 9.4,
            "XLM-RoBERTa-large": 0.9,
            "mT5-base": 14.82,
            "mT5-small": 27.9
        }
    }
}