\begin{longtable}{| l | l | l | l | l |}
\caption{Porównanie wyników na zadaniu zgadywania zamaskowanego słowa dla poszczególnych podzbiorów na zbiorze zadań GLUE Benchmark - część 5.}\label{table:glue_lm_gap_feature_validation_comparing_4}
    \\
    \hline
    \textbf{LM-GAP} & \textbf{F1} & \textbf{F2} & \textbf{F3} & \textbf{F4} \\
    \hline
    \multicolumn{5}{| l |}{\textbf{ALBERT-base}} \\
    \hline
    1575.85 & 859.58 ↑ & 845.68 ↑ & 1016.11 ↑ & 983.57 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{ALBERT-large}} \\
    \hline
    933.39 & 495.65 ↑ & 489.68 ↑ & 604.09 ↑ & 583.7 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{ALBERT-xlarge}} \\
    \hline
    730.84 & 383.75 ↑ & 380.25 ↑ & 480.04 ↑ & 456.17 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{ALBERT-xxlarge}} \\
    \hline
    746.99 & 389.7 ↑ & 388.6 ↑ & 486.56 ↑ & 452.69 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{BERT-base-cased}} \\
    \hline
    172.1 & 53.88 ↑ & 52.91 ↑ & 42.29 ↑ & 49.37 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{BERT-base-multilingual-cased}} \\
    \hline
    635.84 & 222.09 ↑ & 220.7 ↑ & 160.15 ↑ & 190.28 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{BERT-base-multilingual-uncased}} \\
    \hline
    1477.05 & 821.94 ↑ & 812.55 ↑ & 975.49 ↑ & 963.31 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{BERT-base-uncased}} \\
    \hline
    804.25 & 433.18 ↑ & 431.82 ↑ & 538.22 ↑ & 501.5 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{BERT-large-cased}} \\
    \hline
    125.73 & 38.02 ↑ & 37.41 ↑ & 30.41 ↑ & 35.7 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{BERT-large-uncased}} \\
    \hline
    670.67 & 354.24 ↑ & 353.26 ↑ & 449.29 ↑ & 421.15 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{BERT-medium-uncased}} \\
    \hline
    1260.64 & 710.65 ↑ & 707.25 ↑ & 826.27 ↑ & 772.36 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{BERT-mini-uncased}} \\
    \hline
    2714.98 & 1608.94 ↑ & 1597.97 ↑ & 1818.08 ↑ & 1722.99 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{BERT-small-uncased}} \\
    \hline
    1577.66 & 901.05 ↑ & 899.41 ↑ & 1045.82 ↑ & 979.1 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{BERT-tiny-uncased}} \\
    \hline
    7540.61 & 4882.3 ↑ & 4849.03 ↑ & 5191.61 ↑ & 5297.11 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{BioGPT}} \\
    \hline
    9377.07 & 6277.82 ↑ & 6098.43 ↑ & 7003.12 ↑ & 8277.63 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{BioMed-RoBERTa-base}} \\
    \hline
    312.17 & 100.99 ↑ & 97.9 ↑ & 78.5 ↑ & 102.58 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{ByT5-base}} \\
    \hline
    855967.36 & 855652.43 ↑ & 854362.77 ↑ & 869721.17 ↓ & 880084.42 ↓ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{ByT5-small}} \\
    \hline
    584584.35 & 569406.93 ↑ & 589677.63 ↓ & 535945.73 ↑ & 453436.91 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{CamemBERT-base}} \\
    \hline
    3469.33 & 1555.43 ↑ & 1488.0 ↑ & 1208.13 ↑ & 1742.38 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{Cerebras-GPT-111M}} \\
    \hline
    7638.67 & 5507.45 ↑ & 5439.32 ↑ & 6378.73 ↑ & 6240.43 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{Cerebras-GPT-256M}} \\
    \hline
    5563.25 & 3876.05 ↑ & 3842.05 ↑ & 4576.28 ↑ & 4450.82 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{ClinicalBERT}} \\
    \hline
    4443.6 & 2555.83 ↑ & 2550.79 ↑ & 2701.98 ↑ & 2691.09 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{CodeBERT-base}} \\
    \hline
    1174.66 & 431.39 ↑ & 417.63 ↑ & 329.5 ↑ & 469.75 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{DistilBERT-base-cased}} \\
    \hline
    494.41 & 175.31 ↑ & 170.24 ↑ & 145.73 ↑ & 182.19 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{DistilBERT-base-uncased}} \\
    \hline
    1303.64 & 732.04 ↑ & 728.44 ↑ & 895.66 ↑ & 862.69 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{DistilGPT-2}} \\
    \hline
    9714.72 & 8003.24 ↑ & 8132.55 ↑ & 10020.75 ↓ & 8155.39 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{DistilRoBERTa-base}} \\
    \hline
    179.69 & 57.48 ↑ & 55.71 ↑ & 46.67 ↑ & 58.85 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{FLAN-T5-base}} \\
    \hline
    118104.57 & 102844.44 ↑ & 103547.86 ↑ & 108979.27 ↑ & 99615.55 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{FLAN-T5-large}} \\
    \hline
    562529.86 & 544508.62 ↑ & 557327.63 ↑ & 531030.09 ↑ & 476427.6 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{FLAN-T5-small}} \\
    \hline
    324604.41 & 295661.85 ↑ & 298656.92 ↑ & 300537.36 ↑ & 279213.07 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{FinBERT}} \\
    \hline
    3527.82 & 1999.91 ↑ & 2000.97 ↑ & 2103.9 ↑ & 2058.32 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{GPT-2-base}} \\
    \hline
    4700.32 & 3343.47 ↑ & 3313.8 ↑ & 4101.32 ↑ & 3933.96 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{GPT-2-large}} \\
    \hline
    2127.39 & 1224.22 ↑ & 1179.2 ↑ & 1325.5 ↑ & 1553.77 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{GPT-2-medium}} \\
    \hline
    2250.94 & 1271.31 ↑ & 1230.26 ↑ & 1366.97 ↑ & 1614.11 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{GPT-Neo-125M}} \\
    \hline
    4373.95 & 2897.85 ↑ & 2835.14 ↑ & 3489.6 ↑ & 3665.5 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{GPT-fr-base}} \\
    \hline
    46362.46 & 37816.86 ↑ & 36900.78 ↑ & 44148.34 ↑ & 47040.77 ↓ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{GPT-fr-small}} \\
    \hline
    38199.7 & 31389.7 ↑ & 30685.23 ↑ & 37153.35 ↑ & 38487.67 ↓ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{German-BERT-base-cased}} \\
    \hline
    27859.71 & 19736.33 ↑ & 19619.63 ↑ & 18727.73 ↑ & 18966.49 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{LongT5-Local-base}} \\
    \hline
    1008375.97 & 1006936.42 ↑ & 1008528.52 ↓ & 1007120.35 ↑ & 1005126.47 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{LongT5-TGlobal-base}} \\
    \hline
    211414.92 & 200433.32 ↑ & 206355.61 ↑ & 203422.46 ↑ & 176615.64 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{MiniLM-L12-H384-RoBERTa-large}} \\
    \hline
    875114.46 & 866101.26 ↑ & 873279.08 ↑ & 876661.25 ↓ & 880187.61 ↓ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{MiniLM-L12-H384-XLMR-Large}} \\
    \hline
    876508.94 & 873256.38 ↑ & 876534.53 ↓ & 867312.77 ↑ & 863126.46 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{MiniLM-L6-H384-BERT-base-uncased}} \\
    \hline
    869300.22 & 872387.13 ↓ & 867070.63 ↑ & 870906.67 ↓ & 875040.93 ↓ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{MiniLM-L6-H384-BERT-large-uncased}} \\
    \hline
    869819.11 & 868701.96 ↑ & 873988.97 ↓ & 873013.85 ↓ & 884605.15 ↓ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{MiniLM-L6-H384-RoBERTa-large}} \\
    \hline
    875278.91 & 871705.0 ↑ & 863734.95 ↑ & 871957.84 ↑ & 879659.72 ↓ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{MiniLM-L6-H384-XLMR-Large}} \\
    \hline
    867987.18 & 862587.99 ↑ & 873615.33 ↓ & 876225.67 ↓ & 858132.12 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{MiniLM-L6-H768-BERT-base-uncased}} \\
    \hline
    870332.35 & 871119.57 ↓ & 872077.93 ↓ & 871227.28 ↓ & 865747.33 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{MiniLM-L6-H768-BERT-large-uncased}} \\
    \hline
    873852.3 & 873430.97 ↑ & 869865.43 ↑ & 874618.88 ↓ & 871109.14 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{MiniLM-L6-H768-RoBERTa-large}} \\
    \hline
    869124.25 & 871025.79 ↓ & 873970.6 ↓ & 872534.62 ↓ & 863185.37 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{MobileBERT-uncased}} \\
    \hline
    913.53 & 495.61 ↑ & 495.39 ↑ & 601.75 ↑ & 555.67 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{OPT-125M}} \\
    \hline
    2021.62 & 1013.53 ↑ & 949.43 ↑ & 1002.84 ↑ & 1506.34 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{OPT-350M}} \\
    \hline
    1767.23 & 889.2 ↑ & 832.8 ↑ & 879.66 ↑ & 1304.39 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{PolishGPT-2-large}} \\
    \hline
    47767.4 & 38522.46 ↑ & 37518.43 ↑ & 43549.34 ↑ & 48149.25 ↓ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{PolishGPT-2-medium}} \\
    \hline
    58446.34 & 48387.94 ↑ & 47372.84 ↑ & 53669.82 ↑ & 56682.19 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{PolishGPT-2-small}} \\
    \hline
    79278.56 & 69552.19 ↑ & 68170.94 ↑ & 74085.57 ↑ & 78097.59 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{PolishRoBERT-base}} \\
    \hline
    12559.89 & 7160.62 ↑ & 6963.75 ↑ & 6144.75 ↑ & 7537.74 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{Pythia-160M}} \\
    \hline
    5102.75 & 3399.84 ↑ & 3347.24 ↑ & 3936.37 ↑ & 4026.91 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{Pythia-160M-deduped}} \\
    \hline
    5450.9 & 3855.28 ↑ & 3744.27 ↑ & 4756.94 ↑ & 5107.67 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{Pythia-410M}} \\
    \hline
    3613.64 & 2374.05 ↑ & 2340.69 ↑ & 2833.97 ↑ & 2880.38 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{Pythia-410M-deduped}} \\
    \hline
    3927.46 & 2719.24 ↑ & 2631.54 ↑ & 3501.06 ↑ & 3840.12 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{Pythia-70M}} \\
    \hline
    7722.04 & 5308.65 ↑ & 5256.68 ↑ & 5894.89 ↑ & 5876.1 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{Pythia-70M-deduped}} \\
    \hline
    8823.05 & 6466.2 ↑ & 6338.0 ↑ & 7728.94 ↑ & 8285.71 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{RoBERTa-base}} \\
    \hline
    91.91 & 27.81 ↑ & 26.99 ↑ & 23.43 ↑ & 29.13 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{RoBERTa-large}} \\
    \hline
    69.39 & 20.75 ↑ & 20.2 ↑ & 17.88 ↑ & 21.98 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{SciBERT-cased}} \\
    \hline
    2072.33 & 1128.14 ↑ & 1127.01 ↑ & 1233.7 ↑ & 1197.21 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{SciBERT-uncased}} \\
    \hline
    1681.68 & 922.2 ↑ & 925.51 ↑ & 1034.52 ↑ & 993.02 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{SportsBERT}} \\
    \hline
    4407.71 & 2581.71 ↑ & 2545.31 ↑ & 2919.56 ↑ & 2975.01 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{Switch-base-8}} \\
    \hline
    349.97 & 146.85 ↑ & 141.44 ↑ & 174.76 ↑ & 223.17 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{T5-base}} \\
    \hline
    274.69 & 92.54 ↑ & 88.91 ↑ & 79.67 ↑ & 110.83 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{T5-base-v1.1}} \\
    \hline
    267972.65 & 191596.64 ↑ & 190655.61 ↑ & 139083.46 ↑ & 166654.25 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{T5-base-v1.1-lm-adapt}} \\
    \hline
    63737.13 & 43023.57 ↑ & 39987.04 ↑ & 44128.37 ↑ & 79093.78 ↓ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{T5-efficient-base}} \\
    \hline
    124430.99 & 77754.88 ↑ & 74474.17 ↑ & 66952.47 ↑ & 100508.32 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{T5-efficient-large}} \\
    \hline
    84932.1 & 48825.38 ↑ & 46818.66 ↑ & 35628.36 ↑ & 53773.89 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{T5-efficient-mini}} \\
    \hline
    510519.38 & 494234.32 ↑ & 506528.54 ↑ & 467697.08 ↑ & 417700.67 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{T5-efficient-small}} \\
    \hline
    277887.45 & 208626.43 ↑ & 208851.73 ↑ & 158538.6 ↑ & 181384.81 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{T5-efficient-tiny}} \\
    \hline
    418979.06 & 386930.14 ↑ & 397446.81 ↑ & 358850.81 ↑ & 325728.88 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{T5-large}} \\
    \hline
    192.32 & 63.02 ↑ & 60.67 ↑ & 55.31 ↑ & 75.66 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{T5-large-v1.1}} \\
    \hline
    415089.1 & 365856.33 ↑ & 361375.54 ↑ & 338934.23 ↑ & 394373.02 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{T5-large-v1.1-lm-adapt}} \\
    \hline
    300663.1 & 205059.54 ↑ & 204294.93 ↑ & 140931.46 ↑ & 183200.69 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{T5-small}} \\
    \hline
    782.87 & 296.48 ↑ & 286.61 ↑ & 232.36 ↑ & 309.26 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{T5-small-v1.1}} \\
    \hline
    282303.55 & 208199.0 ↑ & 211444.4 ↑ & 145859.48 ↑ & 147124.15 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{T5-small-v1.1-lm-adapt}} \\
    \hline
    233990.72 & 179763.67 ↑ & 178212.68 ↑ & 148893.54 ↑ & 181358.62 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{XLM-100-lang}} \\
    \hline
    15046.79 & 7833.3 ↑ & 8872.48 ↑ & 5439.96 ↑ & 2573.34 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{XLM-17-lang}} \\
    \hline
    52129.25 & 30327.56 ↑ & 36953.35 ↑ & 17683.57 ↑ & 6069.32 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{XLM-RoBERTa-base}} \\
    \hline
    193.88 & 60.34 ↑ & 57.53 ↑ & 48.78 ↑ & 70.52 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{XLM-RoBERTa-large}} \\
    \hline
    138.83 & 42.43 ↑ & 40.52 ↑ & 35.07 ↑ & 49.64 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{XLM-en}} \\
    \hline
    428281.1 & 433319.52 ↓ & 432497.64 ↓ & 446043.22 ↓ & 431257.54 ↓ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{mT5-base}} \\
    \hline
    2491.98 & 1119.53 ↑ & 1090.0 ↑ & 965.07 ↑ & 1215.71 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{mT5-large}} \\
    \hline
    1615.69 & 805.95 ↑ & 772.19 ↑ & 920.66 ↑ & 1196.09 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{mT5-small}} \\
    \hline
    3976.32 & 2103.12 ↑ & 2020.1 ↑ & 2270.04 ↑ & 2824.76 ↑ \\
    \hline
    \multicolumn{5}{| l |}{\textbf{Średnia}} \\
    \hline
    171540.4 & 162530.82 ↑ & 163278.06 ↑ & 158932.7 ↑ & 158680.23 ↑ \\
    \hline
\end{longtable}

