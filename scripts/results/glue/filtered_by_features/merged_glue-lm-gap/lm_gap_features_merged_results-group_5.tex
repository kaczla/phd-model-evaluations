\begin{longtable}{| l | l | l | l | l | l |}
\caption{Porównanie wyników na zadaniu zgadywania zamaskowanego słowa dla poszczególnych podzbiorów na zbiorze zadań GLUE Benchmark - część 6.}\label{table:glue_lm_gap_feature_validation_comparing_5}
    \\
    \hline
    \textbf{LM-GAP} & \textbf{G1} & \textbf{G2} & \textbf{G3} & \textbf{G4} & \textbf{G5} \\
    \hline
    \multicolumn{6}{| l |}{\textbf{ALBERT-base}} \\
    \hline
    1575.85 & 819.81 ↑ & 873.14 ↑ & 1944.42 ↓ & 642.75 ↑ & 522.06 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{ALBERT-large}} \\
    \hline
    933.39 & 459.6 ↑ & 499.56 ↑ & 1178.0 ↓ & 348.03 ↑ & 286.19 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{ALBERT-xlarge}} \\
    \hline
    730.84 & 349.27 ↑ & 376.8 ↑ & 937.67 ↓ & 258.52 ↑ & 207.22 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{ALBERT-xxlarge}} \\
    \hline
    746.99 & 349.45 ↑ & 380.98 ↑ & 966.1 ↓ & 247.79 ↑ & 196.68 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{BERT-base-cased}} \\
    \hline
    172.1 & 109.78 ↑ & 112.66 ↑ & 192.1 ↓ & 96.98 ↑ & 88.2 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{BERT-base-multilingual-cased}} \\
    \hline
    635.84 & 438.95 ↑ & 450.92 ↑ & 709.56 ↓ & 395.51 ↑ & 335.64 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{BERT-base-multilingual-uncased}} \\
    \hline
    1477.05 & 768.33 ↑ & 822.97 ↑ & 1856.29 ↓ & 590.4 ↑ & 450.82 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{BERT-base-uncased}} \\
    \hline
    804.25 & 377.16 ↑ & 409.78 ↑ & 1045.11 ↓ & 270.75 ↑ & 224.52 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{BERT-large-cased}} \\
    \hline
    125.73 & 79.05 ↑ & 81.63 ↑ & 140.47 ↓ & 69.37 ↑ & 66.93 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{BERT-large-uncased}} \\
    \hline
    670.67 & 311.44 ↑ & 337.62 ↑ & 874.41 ↓ & 219.65 ↑ & 176.39 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{BERT-medium-uncased}} \\
    \hline
    1260.64 & 610.66 ↑ & 664.25 ↑ & 1628.05 ↓ & 447.85 ↑ & 344.38 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{BERT-mini-uncased}} \\
    \hline
    2714.98 & 1470.89 ↑ & 1590.24 ↑ & 3378.59 ↓ & 1142.43 ↑ & 834.06 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{BERT-small-uncased}} \\
    \hline
    1577.66 & 796.38 ↑ & 866.43 ↑ & 2006.4 ↓ & 596.79 ↑ & 427.05 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{BERT-tiny-uncased}} \\
    \hline
    7540.61 & 4612.85 ↑ & 4818.0 ↑ & 8872.84 ↓ & 3911.77 ↑ & 3238.15 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{BioGPT}} \\
    \hline
    9377.07 & 6212.09 ↑ & 6510.86 ↑ & 10714.85 ↓ & 5260.25 ↑ & 4404.68 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{BioMed-RoBERTa-base}} \\
    \hline
    312.17 & 213.85 ↑ & 214.82 ↑ & 333.11 ↓ & 203.17 ↑ & 221.09 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{ByT5-base}} \\
    \hline
    855967.36 & 865133.31 ↓ & 864938.78 ↓ & 856590.35 ↓ & 861122.66 ↓ & 885485.79 ↓ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{ByT5-small}} \\
    \hline
    584584.35 & 521543.7 ↑ & 543397.55 ↑ & 634814.1 ↓ & 451109.23 ↑ & 427147.49 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{CamemBERT-base}} \\
    \hline
    3469.33 & 3179.25 ↑ & 3059.58 ↑ & 3258.77 ↑ & 3628.71 ↓ & 4365.12 ↓ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{Cerebras-GPT-111M}} \\
    \hline
    7638.67 & 4401.04 ↑ & 4721.07 ↑ & 9372.62 ↓ & 3471.06 ↑ & 2671.27 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{Cerebras-GPT-256M}} \\
    \hline
    5563.25 & 3144.25 ↑ & 3374.71 ↑ & 6803.24 ↓ & 2478.63 ↑ & 1905.09 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{ClinicalBERT}} \\
    \hline
    4443.6 & 2345.99 ↑ & 2557.58 ↑ & 5611.66 ↓ & 1825.32 ↑ & 1293.06 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{CodeBERT-base}} \\
    \hline
    1174.66 & 820.59 ↑ & 823.8 ↑ & 1231.22 ↓ & 812.97 ↑ & 806.16 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{DistilBERT-base-cased}} \\
    \hline
    494.41 & 330.28 ↑ & 338.86 ↑ & 544.84 ↓ & 293.09 ↑ & 289.57 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{DistilBERT-base-uncased}} \\
    \hline
    1303.64 & 675.27 ↑ & 727.45 ↑ & 1629.4 ↓ & 516.82 ↑ & 422.35 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{DistilGPT-2}} \\
    \hline
    9714.72 & 4626.61 ↑ & 5085.33 ↑ & 12897.0 ↓ & 3236.31 ↑ & 2334.35 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{DistilRoBERTa-base}} \\
    \hline
    179.69 & 123.11 ↑ & 123.07 ↑ & 189.48 ↓ & 117.35 ↑ & 133.87 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{FLAN-T5-base}} \\
    \hline
    118104.57 & 102574.95 ↑ & 107563.62 ↑ & 126375.06 ↓ & 93860.31 ↑ & 75223.77 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{FLAN-T5-large}} \\
    \hline
    562529.86 & 539568.19 ↑ & 545044.04 ↑ & 578178.38 ↓ & 523532.47 ↑ & 482551.36 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{FLAN-T5-small}} \\
    \hline
    324604.41 & 289991.38 ↑ & 299116.34 ↑ & 345240.21 ↓ & 275099.34 ↑ & 234430.29 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{FinBERT}} \\
    \hline
    3527.82 & 1878.91 ↑ & 2015.12 ↑ & 4510.46 ↓ & 1410.07 ↑ & 1131.61 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{GPT-2-base}} \\
    \hline
    4700.32 & 2468.66 ↑ & 2657.47 ↑ & 5896.79 ↓ & 1892.84 ↑ & 1556.83 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{GPT-2-large}} \\
    \hline
    2127.39 & 1274.6 ↑ & 1344.74 ↑ & 2463.84 ↓ & 1068.61 ↑ & 993.52 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{GPT-2-medium}} \\
    \hline
    2250.94 & 1402.01 ↑ & 1461.79 ↑ & 2575.34 ↓ & 1202.77 ↑ & 1116.95 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{GPT-Neo-125M}} \\
    \hline
    4373.95 & 2546.21 ↑ & 2697.76 ↑ & 5203.7 ↓ & 2080.64 ↑ & 1702.57 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{GPT-fr-base}} \\
    \hline
    46362.46 & 33846.96 ↑ & 36089.45 ↑ & 53135.27 ↓ & 28415.41 ↑ & 21203.83 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{GPT-fr-small}} \\
    \hline
    38199.7 & 26805.62 ↑ & 28510.27 ↑ & 43924.6 ↓ & 22886.31 ↑ & 16922.91 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{German-BERT-base-cased}} \\
    \hline
    27859.71 & 22573.22 ↑ & 23101.76 ↑ & 28963.76 ↓ & 22852.65 ↑ & 19928.3 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{LongT5-Local-base}} \\
    \hline
    1008375.97 & 1006844.6 ↑ & 1007622.29 ↑ & 1009745.4 ↓ & 1007494.75 ↑ & 1001874.0 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{LongT5-TGlobal-base}} \\
    \hline
    211414.92 & 167769.03 ↑ & 178123.75 ↑ & 236796.34 ↓ & 149213.64 ↑ & 109475.71 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{MiniLM-L12-H384-RoBERTa-large}} \\
    \hline
    875114.46 & 872705.42 ↑ & 857838.53 ↑ & 874430.16 ↑ & 868281.66 ↑ & 871194.77 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{MiniLM-L12-H384-XLMR-Large}} \\
    \hline
    876508.94 & 874316.59 ↑ & 866537.7 ↑ & 867372.93 ↑ & 857365.85 ↑ & 865786.66 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{MiniLM-L6-H384-BERT-base-uncased}} \\
    \hline
    869300.22 & 871459.82 ↓ & 869806.44 ↓ & 871397.24 ↓ & 871385.68 ↓ & 850667.95 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{MiniLM-L6-H384-BERT-large-uncased}} \\
    \hline
    869819.11 & 872201.81 ↓ & 870307.94 ↓ & 873631.35 ↓ & 862069.99 ↑ & 871274.5 ↓ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{MiniLM-L6-H384-RoBERTa-large}} \\
    \hline
    875278.91 & 866903.88 ↑ & 868185.08 ↑ & 872685.57 ↑ & 869922.64 ↑ & 891884.6 ↓ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{MiniLM-L6-H384-XLMR-Large}} \\
    \hline
    867987.18 & 867051.69 ↑ & 869608.95 ↓ & 868063.74 ↓ & 879455.41 ↓ & 867983.14 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{MiniLM-L6-H768-BERT-base-uncased}} \\
    \hline
    870332.35 & 877895.85 ↓ & 881467.09 ↓ & 879233.31 ↓ & 868208.86 ↑ & 869692.03 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{MiniLM-L6-H768-BERT-large-uncased}} \\
    \hline
    873852.3 & 867621.28 ↑ & 868204.49 ↑ & 861824.76 ↑ & 868336.73 ↑ & 897136.7 ↓ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{MiniLM-L6-H768-RoBERTa-large}} \\
    \hline
    869124.25 & 868205.12 ↑ & 870961.96 ↓ & 871780.79 ↓ & 867385.67 ↑ & 841185.85 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{MobileBERT-uncased}} \\
    \hline
    913.53 & 427.28 ↑ & 467.12 ↑ & 1191.83 ↓ & 304.75 ↑ & 241.76 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{OPT-125M}} \\
    \hline
    2021.62 & 1505.56 ↑ & 1507.01 ↑ & 2080.79 ↓ & 1475.67 ↑ & 1619.14 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{OPT-350M}} \\
    \hline
    1767.23 & 1280.84 ↑ & 1288.8 ↑ & 1849.95 ↓ & 1222.57 ↑ & 1348.52 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{PolishGPT-2-large}} \\
    \hline
    47767.4 & 38068.36 ↑ & 38975.11 ↑ & 51065.81 ↓ & 35500.7 ↑ & 30778.23 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{PolishGPT-2-medium}} \\
    \hline
    58446.34 & 44987.37 ↑ & 46822.73 ↑ & 64239.11 ↓ & 41052.84 ↑ & 33402.85 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{PolishGPT-2-small}} \\
    \hline
    79278.56 & 58869.44 ↑ & 61419.11 ↑ & 88145.58 ↓ & 53725.09 ↑ & 43432.59 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{PolishRoBERT-base}} \\
    \hline
    12559.89 & 11149.41 ↑ & 11157.49 ↑ & 12479.24 ↑ & 11642.28 ↑ & 10564.74 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{Pythia-160M}} \\
    \hline
    5102.75 & 2884.11 ↑ & 3099.18 ↑ & 6239.54 ↓ & 2251.24 ↑ & 1774.53 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{Pythia-160M-deduped}} \\
    \hline
    5450.9 & 3014.9 ↑ & 3221.15 ↑ & 6710.94 ↓ & 2364.07 ↑ & 1993.9 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{Pythia-410M}} \\
    \hline
    3613.64 & 1965.06 ↑ & 2141.05 ↑ & 4503.13 ↓ & 1480.1 ↑ & 1085.43 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{Pythia-410M-deduped}} \\
    \hline
    3927.46 & 2149.18 ↑ & 2332.1 ↑ & 4843.59 ↓ & 1613.79 ↑ & 1210.95 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{Pythia-70M}} \\
    \hline
    7722.04 & 4600.55 ↑ & 4949.82 ↑ & 9452.58 ↓ & 3590.42 ↑ & 2658.99 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{Pythia-70M-deduped}} \\
    \hline
    8823.05 & 5085.98 ↑ & 5418.18 ↑ & 10698.15 ↓ & 4024.98 ↑ & 3032.28 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{RoBERTa-base}} \\
    \hline
    91.91 & 60.69 ↑ & 61.59 ↑ & 99.11 ↓ & 55.42 ↑ & 61.96 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{RoBERTa-large}} \\
    \hline
    69.39 & 44.76 ↑ & 45.42 ↑ & 75.2 ↓ & 40.45 ↑ & 43.37 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{SciBERT-cased}} \\
    \hline
    2072.33 & 1013.09 ↑ & 1089.73 ↑ & 2664.57 ↓ & 756.38 ↑ & 597.65 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{SciBERT-uncased}} \\
    \hline
    1681.68 & 830.52 ↑ & 892.1 ↑ & 2163.98 ↓ & 618.04 ↑ & 490.16 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{SportsBERT}} \\
    \hline
    4407.71 & 2659.19 ↑ & 2830.94 ↑ & 5245.61 ↓ & 2220.42 ↑ & 1577.62 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{Switch-base-8}} \\
    \hline
    349.97 & 212.21 ↑ & 220.17 ↑ & 388.04 ↓ & 181.53 ↑ & 186.96 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{T5-base}} \\
    \hline
    274.69 & 189.03 ↑ & 189.42 ↑ & 284.28 ↓ & 183.76 ↑ & 221.48 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{T5-base-v1.1}} \\
    \hline
    267972.65 & 314460.24 ↓ & 304544.66 ↓ & 245053.32 ↑ & 357557.03 ↓ & 377277.65 ↓ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{T5-base-v1.1-lm-adapt}} \\
    \hline
    63737.13 & 80550.97 ↓ & 75130.61 ↓ & 53327.96 ↑ & 105618.9 ↓ & 142244.02 ↓ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{T5-efficient-base}} \\
    \hline
    124430.99 & 166458.39 ↓ & 159789.57 ↓ & 108816.95 ↑ & 195862.37 ↓ & 206993.14 ↓ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{T5-efficient-large}} \\
    \hline
    84932.1 & 111007.98 ↓ & 103135.63 ↓ & 70070.95 ↑ & 152947.37 ↓ & 172024.59 ↓ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{T5-efficient-mini}} \\
    \hline
    510519.38 & 464320.98 ↑ & 472262.96 ↑ & 533770.82 ↓ & 450313.69 ↑ & 401573.6 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{T5-efficient-small}} \\
    \hline
    277887.45 & 296064.48 ↓ & 286624.99 ↓ & 263823.64 ↑ & 323946.04 ↓ & 329505.26 ↓ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{T5-efficient-tiny}} \\
    \hline
    418979.06 & 374798.79 ↑ & 378679.72 ↑ & 443150.32 ↓ & 354093.73 ↑ & 323077.62 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{T5-large}} \\
    \hline
    192.32 & 126.06 ↑ & 126.57 ↑ & 199.06 ↓ & 122.31 ↑ & 155.68 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{T5-large-v1.1}} \\
    \hline
    415089.1 & 452312.24 ↓ & 451047.04 ↓ & 408683.33 ↑ & 462866.49 ↓ & 407957.56 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{T5-large-v1.1-lm-adapt}} \\
    \hline
    300663.1 & 388420.64 ↓ & 369989.9 ↓ & 260024.76 ↑ & 487320.35 ↓ & 593659.53 ↓ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{T5-small}} \\
    \hline
    782.87 & 515.16 ↑ & 518.19 ↑ & 812.27 ↓ & 506.43 ↑ & 593.35 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{T5-small-v1.1}} \\
    \hline
    282303.55 & 305525.13 ↓ & 298092.36 ↓ & 266152.63 ↑ & 341130.3 ↓ & 347239.89 ↓ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{T5-small-v1.1-lm-adapt}} \\
    \hline
    233990.72 & 259807.64 ↓ & 257670.12 ↓ & 222378.97 ↑ & 286071.48 ↓ & 301887.01 ↓ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{XLM-100-lang}} \\
    \hline
    15046.79 & 4542.26 ↑ & 6068.37 ↑ & 30127.81 ↓ & 1622.83 ↑ & 419.44 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{XLM-17-lang}} \\
    \hline
    52129.25 & 16704.67 ↑ & 25321.3 ↑ & 120223.52 ↓ & 4390.46 ↑ & 450.7 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{XLM-RoBERTa-base}} \\
    \hline
    193.88 & 138.29 ↑ & 135.35 ↑ & 193.61 ↑ & 148.27 ↑ & 187.8 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{XLM-RoBERTa-large}} \\
    \hline
    138.83 & 96.8 ↑ & 94.18 ↑ & 138.33 ↑ & 101.45 ↑ & 129.02 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{XLM-en}} \\
    \hline
    428281.1 & 385065.83 ↑ & 394432.04 ↑ & 447229.46 ↓ & 358714.11 ↑ & 327569.93 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{mT5-base}} \\
    \hline
    2491.98 & 1472.05 ↑ & 1484.09 ↑ & 2690.98 ↓ & 1436.11 ↑ & 1726.28 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{mT5-large}} \\
    \hline
    1615.69 & 1056.63 ↑ & 1064.04 ↑ & 1704.47 ↓ & 1018.54 ↑ & 1268.57 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{mT5-small}} \\
    \hline
    3976.32 & 2805.04 ↑ & 2854.31 ↑ & 4175.62 ↓ & 2755.98 ↑ & 2888.97 ↑ \\
    \hline
    \multicolumn{6}{| l |}{\textbf{Średnia}} \\
    \hline
    171540.4 & 169660.63 ↑ & 169817.99 ↑ & 173447.99 ↓ & 170439.84 ↑ & 168988.32 ↑ \\
    \hline
\end{longtable}

