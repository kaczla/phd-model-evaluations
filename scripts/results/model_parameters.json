{
    "roberta-base": {
        "name": "roberta-base",
        "human_name": "RoBERTa-base",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 50265
    },
    "roberta-large": {
        "name": "roberta-large",
        "human_name": "RoBERTa-large",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 24,
        "encoder_hidden_size": 1024,
        "encoder_feedforward_hidden_size": 4096,
        "encoder_number_attention_heads": 16,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 50265
    },
    "bert-base-cased": {
        "name": "bert-base-cased",
        "human_name": "BERT-base-cased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 28996
    },
    "bert-base-uncased": {
        "name": "bert-base-uncased",
        "human_name": "BERT-base-uncased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 30522
    },
    "bert-large-cased": {
        "name": "bert-large-cased",
        "human_name": "BERT-large-cased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 24,
        "encoder_hidden_size": 1024,
        "encoder_feedforward_hidden_size": 4096,
        "encoder_number_attention_heads": 16,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 28996
    },
    "bert-large-uncased": {
        "name": "bert-large-uncased",
        "human_name": "BERT-large-uncased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 24,
        "encoder_hidden_size": 1024,
        "encoder_feedforward_hidden_size": 4096,
        "encoder_number_attention_heads": 16,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 30522
    },
    "prajjwal1/bert-tiny": {
        "name": "prajjwal1/bert-tiny",
        "human_name": "BERT-tiny-uncased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 2,
        "encoder_hidden_size": 128,
        "encoder_feedforward_hidden_size": 512,
        "encoder_number_attention_heads": 2,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 30522
    },
    "prajjwal1/bert-mini": {
        "name": "prajjwal1/bert-mini",
        "human_name": "BERT-mini-uncased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 4,
        "encoder_hidden_size": 256,
        "encoder_feedforward_hidden_size": 1024,
        "encoder_number_attention_heads": 4,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 30522
    },
    "prajjwal1/bert-small": {
        "name": "prajjwal1/bert-small",
        "human_name": "BERT-small-uncased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 4,
        "encoder_hidden_size": 512,
        "encoder_feedforward_hidden_size": 2048,
        "encoder_number_attention_heads": 8,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 30522
    },
    "prajjwal1/bert-medium": {
        "name": "prajjwal1/bert-medium",
        "human_name": "BERT-medium-uncased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 8,
        "encoder_hidden_size": 512,
        "encoder_feedforward_hidden_size": 2048,
        "encoder_number_attention_heads": 8,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 30522
    },
    "distilroberta-base": {
        "name": "distilroberta-base",
        "human_name": "DistilRoBERTa-base",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 6,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 50265
    },
    "distilbert-base-cased": {
        "name": "distilbert-base-cased",
        "human_name": "DistilBERT-base-cased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 6,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 28996
    },
    "distilbert-base-uncased": {
        "name": "distilbert-base-uncased",
        "human_name": "DistilBERT-base-uncased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 6,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 30522
    },
    "albert-base-v2": {
        "name": "albert-base-v2",
        "human_name": "ALBERT-base",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 30000
    },
    "albert-large-v2": {
        "name": "albert-large-v2",
        "human_name": "ALBERT-large",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 24,
        "encoder_hidden_size": 1024,
        "encoder_feedforward_hidden_size": 4096,
        "encoder_number_attention_heads": 16,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 30000
    },
    "google/mobilebert-uncased": {
        "name": "google/mobilebert-uncased",
        "human_name": "MobileBERT-uncased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 24,
        "encoder_hidden_size": 512,
        "encoder_feedforward_hidden_size": 512,
        "encoder_number_attention_heads": 4,
        "encoder_attention_size": 128,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 30522
    },
    "yiyanghkust/finbert-pretrain": {
        "name": "yiyanghkust/finbert-pretrain",
        "human_name": "FinBERT",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 30873
    },
    "allenai/scibert_scivocab_uncased": {
        "name": "allenai/scibert_scivocab_uncased",
        "human_name": "SciBERT-uncased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 31090
    },
    "allenai/scibert_scivocab_cased": {
        "name": "allenai/scibert_scivocab_cased",
        "human_name": "SciBERT-cased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 31116
    },
    "allenai/biomed_roberta_base": {
        "name": "allenai/biomed_roberta_base",
        "human_name": "BioMed-RoBERTa-base",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 50265
    },
    "emilyalsentzer/Bio_ClinicalBERT": {
        "name": "emilyalsentzer/Bio_ClinicalBERT",
        "human_name": "ClinicalBERT",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 28996
    },
    "microsoft/codebert-base-mlm": {
        "name": "microsoft/codebert-base-mlm",
        "human_name": "CodeBERT-base",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 50265
    },
    "xlm-roberta-base": {
        "name": "xlm-roberta-base",
        "human_name": "XLM-RoBERTa-base",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 250002
    },
    "xlm-roberta-large": {
        "name": "xlm-roberta-large",
        "human_name": "XLM-RoBERTa-large",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 24,
        "encoder_hidden_size": 1024,
        "encoder_feedforward_hidden_size": 4096,
        "encoder_number_attention_heads": 16,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 250002
    },
    "bert-base-multilingual-uncased": {
        "name": "bert-base-multilingual-uncased",
        "human_name": "BERT-base-multilingual-uncased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 105879
    },
    "bert-base-multilingual-cased": {
        "name": "bert-base-multilingual-cased",
        "human_name": "BERT-base-multilingual-cased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 119547
    },
    "allenai/longformer-base-4096": {
        "name": "allenai/longformer-base-4096",
        "human_name": "Longformer-base",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 4096,
        "vocab_size": 50265
    },
    "allenai/longformer-large-4096": {
        "name": "allenai/longformer-large-4096",
        "human_name": "Longformer-large",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 24,
        "encoder_hidden_size": 1024,
        "encoder_feedforward_hidden_size": 4096,
        "encoder_number_attention_heads": 16,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 4096,
        "vocab_size": 50265
    },
    "camembert-base": {
        "name": "camembert-base",
        "human_name": "CamemBERT-base",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 32005
    },
    "sdadas/polish-roberta-base-v1": {
        "name": "sdadas/polish-roberta-base-v1",
        "human_name": "PolishRoBERT-base",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 514,
        "vocab_size": 50001
    },
    "bert-base-german-cased": {
        "name": "bert-base-german-cased",
        "human_name": "German-BERT-base-cased",
        "architecture_type": "encoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 0,
        "decoder_hidden_size": 0,
        "decoder_feedforward_hidden_size": 0,
        "decoder_number_attention_heads": 0,
        "decoder_attention_size": 0,
        "sequence_length": 512,
        "vocab_size": 30000
    },
    "gpt2": {
        "name": "gpt2",
        "human_name": "GPT-2-base",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 1024,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 1024,
        "vocab_size": 50257
    },
    "gpt2-medium": {
        "name": "gpt2-medium",
        "human_name": "GPT-2-medium",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 24,
        "decoder_hidden_size": 1024,
        "decoder_feedforward_hidden_size": 1024,
        "decoder_number_attention_heads": 16,
        "decoder_attention_size": 64,
        "sequence_length": 1024,
        "vocab_size": 50257
    },
    "EleutherAI/gpt-neo-125M": {
        "name": "EleutherAI/gpt-neo-125M",
        "human_name": "GPT-Neo-125M",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 3072,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 2048,
        "vocab_size": 50257
    },
    "EleutherAI/pythia-70m": {
        "name": "EleutherAI/pythia-70m",
        "human_name": "Pythia-70M",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 6,
        "decoder_hidden_size": 512,
        "decoder_feedforward_hidden_size": 2048,
        "decoder_number_attention_heads": 8,
        "decoder_attention_size": 64,
        "sequence_length": 2048,
        "vocab_size": 50254
    },
    "EleutherAI/pythia-70m-deduped": {
        "name": "EleutherAI/pythia-70m-deduped",
        "human_name": "Pythia-70M-deduped",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 6,
        "decoder_hidden_size": 512,
        "decoder_feedforward_hidden_size": 2048,
        "decoder_number_attention_heads": 8,
        "decoder_attention_size": 64,
        "sequence_length": 2048,
        "vocab_size": 50254
    },
    "EleutherAI/pythia-160m": {
        "name": "EleutherAI/pythia-160m",
        "human_name": "Pythia-160M",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 3072,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 2048,
        "vocab_size": 50254
    },
    "EleutherAI/pythia-160m-deduped": {
        "name": "EleutherAI/pythia-160m-deduped",
        "human_name": "Pythia-160M-deduped",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 3072,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 2048,
        "vocab_size": 50254
    },
    "EleutherAI/pythia-410m": {
        "name": "EleutherAI/pythia-410m",
        "human_name": "Pythia-410M",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 24,
        "decoder_hidden_size": 1024,
        "decoder_feedforward_hidden_size": 4096,
        "decoder_number_attention_heads": 16,
        "decoder_attention_size": 64,
        "sequence_length": 2048,
        "vocab_size": 50254
    },
    "EleutherAI/pythia-410m-deduped": {
        "name": "EleutherAI/pythia-410m-deduped",
        "human_name": "Pythia-410M-deduped",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 24,
        "decoder_hidden_size": 1024,
        "decoder_feedforward_hidden_size": 4096,
        "decoder_number_attention_heads": 16,
        "decoder_attention_size": 64,
        "sequence_length": 2048,
        "vocab_size": 50254
    },
    "facebook/opt-125m": {
        "name": "facebook/opt-125m",
        "human_name": "OPT-125M",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 3072,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 2048,
        "vocab_size": 50265
    },
    "facebook/opt-350m": {
        "name": "facebook/opt-350m",
        "human_name": "OPT-350M",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 24,
        "decoder_hidden_size": 1024,
        "decoder_feedforward_hidden_size": 4096,
        "decoder_number_attention_heads": 16,
        "decoder_attention_size": 64,
        "sequence_length": 2048,
        "vocab_size": 50265
    },
    "cerebras/Cerebras-GPT-111M": {
        "name": "cerebras/Cerebras-GPT-111M",
        "human_name": "Cerebras-GPT-111M",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 10,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 3072,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 2048,
        "vocab_size": 50257
    },
    "cerebras/Cerebras-GPT-256M": {
        "name": "cerebras/Cerebras-GPT-256M",
        "human_name": "Cerebras-GPT-256M",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 14,
        "decoder_hidden_size": 1088,
        "decoder_feedforward_hidden_size": 4352,
        "decoder_number_attention_heads": 17,
        "decoder_attention_size": 64,
        "sequence_length": 2048,
        "vocab_size": 50257
    },
    "distilgpt2": {
        "name": "distilgpt2",
        "human_name": "DistilGPT-2",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 6,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 1024,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 1024,
        "vocab_size": 50257
    },
    "microsoft/biogpt": {
        "name": "microsoft/biogpt",
        "human_name": "BioGPT",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 24,
        "decoder_hidden_size": 1024,
        "decoder_feedforward_hidden_size": 4096,
        "decoder_number_attention_heads": 16,
        "decoder_attention_size": 64,
        "sequence_length": 1024,
        "vocab_size": 42384
    },
    "asi/gpt-fr-cased-small": {
        "name": "asi/gpt-fr-cased-small",
        "human_name": "GPT-fr-small",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 1024,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 1024,
        "vocab_size": 50000
    },
    "sdadas/polish-gpt2-small": {
        "name": "sdadas/polish-gpt2-small",
        "human_name": "PolishGPT-2-small",
        "architecture_type": "decoder",
        "number_parameters": 0,
        "encoder_number_layers": 0,
        "encoder_hidden_size": 0,
        "encoder_feedforward_hidden_size": 0,
        "encoder_number_attention_heads": 0,
        "encoder_attention_size": 0,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 3072,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 2048,
        "vocab_size": 51200
    },
    "t5-small": {
        "name": "t5-small",
        "human_name": "T5-small",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 6,
        "encoder_hidden_size": 512,
        "encoder_feedforward_hidden_size": 2048,
        "encoder_number_attention_heads": 8,
        "encoder_attention_size": 64,
        "decoder_number_layers": 6,
        "decoder_hidden_size": 512,
        "decoder_feedforward_hidden_size": 2048,
        "decoder_number_attention_heads": 8,
        "decoder_attention_size": 64,
        "sequence_length": 512,
        "vocab_size": 32100
    },
    "t5-base": {
        "name": "t5-base",
        "human_name": "T5-base",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 3072,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 512,
        "vocab_size": 32100
    },
    "google/t5-v1_1-small": {
        "name": "google/t5-v1_1-small",
        "human_name": "T5-small-v1.1",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 8,
        "encoder_hidden_size": 512,
        "encoder_feedforward_hidden_size": 1024,
        "encoder_number_attention_heads": 6,
        "encoder_attention_size": 85,
        "decoder_number_layers": 8,
        "decoder_hidden_size": 512,
        "decoder_feedforward_hidden_size": 1024,
        "decoder_number_attention_heads": 6,
        "decoder_attention_size": 85,
        "sequence_length": 512,
        "vocab_size": 32100
    },
    "google/t5-v1_1-base": {
        "name": "google/t5-v1_1-base",
        "human_name": "T5-base-v1.1",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 2048,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 2048,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 512,
        "vocab_size": 32100
    },
    "google/t5-small-lm-adapt": {
        "name": "google/t5-small-lm-adapt",
        "human_name": "T5-small-v1.1-lm-adapt",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 8,
        "encoder_hidden_size": 512,
        "encoder_feedforward_hidden_size": 1024,
        "encoder_number_attention_heads": 6,
        "encoder_attention_size": 85,
        "decoder_number_layers": 8,
        "decoder_hidden_size": 512,
        "decoder_feedforward_hidden_size": 1024,
        "decoder_number_attention_heads": 6,
        "decoder_attention_size": 85,
        "sequence_length": 512,
        "vocab_size": 32100
    },
    "google/t5-base-lm-adapt": {
        "name": "google/t5-base-lm-adapt",
        "human_name": "T5-base-v1.1-lm-adapt",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 2048,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 2048,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 512,
        "vocab_size": 32100
    },
    "google/t5-efficient-tiny": {
        "name": "google/t5-efficient-tiny",
        "human_name": "T5-efficient-tiny",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 4,
        "encoder_hidden_size": 256,
        "encoder_feedforward_hidden_size": 1024,
        "encoder_number_attention_heads": 4,
        "encoder_attention_size": 64,
        "decoder_number_layers": 4,
        "decoder_hidden_size": 256,
        "decoder_feedforward_hidden_size": 1024,
        "decoder_number_attention_heads": 4,
        "decoder_attention_size": 64,
        "sequence_length": 512,
        "vocab_size": 32100
    },
    "google/t5-efficient-mini": {
        "name": "google/t5-efficient-mini",
        "human_name": "T5-efficient-mini",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 4,
        "encoder_hidden_size": 384,
        "encoder_feedforward_hidden_size": 1536,
        "encoder_number_attention_heads": 8,
        "encoder_attention_size": 48,
        "decoder_number_layers": 4,
        "decoder_hidden_size": 384,
        "decoder_feedforward_hidden_size": 1536,
        "decoder_number_attention_heads": 8,
        "decoder_attention_size": 48,
        "sequence_length": 512,
        "vocab_size": 32100
    },
    "google/t5-efficient-small": {
        "name": "google/t5-efficient-small",
        "human_name": "T5-efficient-small",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 6,
        "encoder_hidden_size": 512,
        "encoder_feedforward_hidden_size": 2048,
        "encoder_number_attention_heads": 8,
        "encoder_attention_size": 64,
        "decoder_number_layers": 6,
        "decoder_hidden_size": 512,
        "decoder_feedforward_hidden_size": 2048,
        "decoder_number_attention_heads": 8,
        "decoder_attention_size": 64,
        "sequence_length": 512,
        "vocab_size": 32100
    },
    "google/t5-efficient-base": {
        "name": "google/t5-efficient-base",
        "human_name": "T5-efficient-base",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 3072,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 512,
        "vocab_size": 32100
    },
    "google/switch-base-8": {
        "name": "google/switch-base-8",
        "human_name": "Switch-base-8",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 3072,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 3072,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 512,
        "vocab_size": 32100
    },
    "google/flan-t5-small": {
        "name": "google/flan-t5-small",
        "human_name": "FLAN-T5-small",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 8,
        "encoder_hidden_size": 512,
        "encoder_feedforward_hidden_size": 1024,
        "encoder_number_attention_heads": 6,
        "encoder_attention_size": 85,
        "decoder_number_layers": 8,
        "decoder_hidden_size": 512,
        "decoder_feedforward_hidden_size": 1024,
        "decoder_number_attention_heads": 6,
        "decoder_attention_size": 85,
        "sequence_length": 512,
        "vocab_size": 32100
    },
    "google/flan-t5-base": {
        "name": "google/flan-t5-base",
        "human_name": "FLAN-T5-base",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 2048,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 2048,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 512,
        "vocab_size": 32100
    },
    "google/mt5-small": {
        "name": "google/mt5-small",
        "human_name": "mT5-small",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 8,
        "encoder_hidden_size": 512,
        "encoder_feedforward_hidden_size": 1024,
        "encoder_number_attention_heads": 6,
        "encoder_attention_size": 85,
        "decoder_number_layers": 8,
        "decoder_hidden_size": 512,
        "decoder_feedforward_hidden_size": 1024,
        "decoder_number_attention_heads": 6,
        "decoder_attention_size": 85,
        "sequence_length": 1024,
        "vocab_size": 250100
    },
    "google/mt5-base": {
        "name": "google/mt5-base",
        "human_name": "mT5-base",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 2048,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 2048,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 1024,
        "vocab_size": 250100
    },
    "google/byt5-small": {
        "name": "google/byt5-small",
        "human_name": "ByT5-small",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 1472,
        "encoder_feedforward_hidden_size": 3584,
        "encoder_number_attention_heads": 6,
        "encoder_attention_size": 245,
        "decoder_number_layers": 4,
        "decoder_hidden_size": 1472,
        "decoder_feedforward_hidden_size": 3584,
        "decoder_number_attention_heads": 6,
        "decoder_attention_size": 245,
        "sequence_length": 1024,
        "vocab_size": 384
    },
    "google/byt5-base": {
        "name": "google/byt5-base",
        "human_name": "ByT5-base",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 18,
        "encoder_hidden_size": 1536,
        "encoder_feedforward_hidden_size": 3968,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 128,
        "decoder_number_layers": 6,
        "decoder_hidden_size": 1536,
        "decoder_feedforward_hidden_size": 3968,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 128,
        "sequence_length": 1024,
        "vocab_size": 384
    },
    "google/long-t5-tglobal-base": {
        "name": "google/long-t5-tglobal-base",
        "human_name": "LongT5-TGlobal-base",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 2048,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 2048,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 4096,
        "vocab_size": 32100
    },
    "google/long-t5-local-base": {
        "name": "google/long-t5-local-base",
        "human_name": "LongT5-Local-base",
        "architecture_type": "encoder_decoder",
        "number_parameters": 0,
        "encoder_number_layers": 12,
        "encoder_hidden_size": 768,
        "encoder_feedforward_hidden_size": 2048,
        "encoder_number_attention_heads": 12,
        "encoder_attention_size": 64,
        "decoder_number_layers": 12,
        "decoder_hidden_size": 768,
        "decoder_feedforward_hidden_size": 2048,
        "decoder_number_attention_heads": 12,
        "decoder_attention_size": 64,
        "sequence_length": 4096,
        "vocab_size": 32100
    }
}